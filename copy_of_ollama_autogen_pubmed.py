# -*- coding: utf-8 -*-
"""Copy of ollama_autogen_pubmed.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WgWKLH3sdXvzY7bd-ZZqCfZ8lI-pol05
"""

!curl https://ollama.ai/install.sh | sh
!command -v systemctl >/dev/null && sudo systemctl stop ollama

!ollama serve > server.log 2>&1 &

import os
os.environ["OLLAMA_HOST"] = "http://127.0.0.1:11434"
os.environ["LD_LIBRARY_PATH"] = "/usr/lib64-nvidia"

!ollama run mistral > server_log.log 2>&1 &

!pip install litellm[proxy]

from litellm import completion

response = completion(
    model="ollama/mistral",
    messages=[{ "content": "respond in 20 words. who are you?","role": "user"}],
    api_base="http://127.0.0.1:11434",
    stream=True
)
print(response)
for chunk in response:
    print(chunk['choices'][0]['delta'])

!pip install pyautogen

!litellm --model ollama/mistral --api_base "http://127.0.0.1:11434" --debug > server_log_2.log 2>&1 &

from autogen import AssistantAgent, GroupChatManager, UserProxyAgent
from autogen.agentchat import GroupChat
config_list = [
    {
        "model": "ollama/mistral",
        "base_url": "http://localhost:8000",  # litellm compatible endpoint
        "api_key": "NULL",  # just a placeholder
    }
]
llm_config = {"config_list": config_list,}

code_config = {"config_list": config_list,}


admin = UserProxyAgent(
    name="Admin",
    system_message="A human admin. Interact with the planner to discuss the plan. Plan execution needs to be approved by this admin.",
    llm_config=llm_config,
    code_execution_config=False,
)


engineer = AssistantAgent(
    name="Engineer",
    llm_config=code_config,
    system_message="""Engineer. You follow an approved plan. You write python/shell code to solve tasks. Wrap the code in a code block that specifies the script type. The user can't modify your code. So do not suggest incomplete code which requires others to modify. Don't use a code block if it's not intended to be executed by the executor.
Don't include multiple code blocks in one response. Do not ask others to copy and paste the result. Check the execution result returned by the executor.
If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.
""",
)
planner = AssistantAgent(
    name="Planner",
    system_message="""Planner. Suggest a plan. Revise the plan based on feedback from admin and critic, until admin approval.
The plan may involve an engineer who can write code and a scientist who doesn't write code.
Explain the plan first. Be clear which step is performed by an engineer, and which step is performed by a scientist.
""",
    llm_config=llm_config,
)
executor = UserProxyAgent(
    name="Executor",
    system_message="Executor. Execute the code written by the engineer and report the result.",
    human_input_mode="NEVER",
    llm_config=llm_config,
    code_execution_config={"last_n_messages": 3, "work_dir": "paper"},
)
critic = AssistantAgent(
    name="Critic",
    system_message="Critic. Double check plan, claims, code from other agents and provide feedback. Check whether the plan includes adding verifiable info such as source URL.",
    llm_config=llm_config,
)
groupchat = GroupChat(
    agents=[admin, engineer, planner, executor, critic],
    messages=[],
    max_round=50,
)
manager = GroupChatManager(groupchat=groupchat, llm_config=llm_config)


admin.initiate_chat(
    manager,
    message=""" Create a python app to predict stock prices
""",
)


from autogen import AssistantAgent, GroupChatManager, UserProxyAgent
from autogen.agentchat import GroupChat

# Configuration for connecting to a model server
config_list = [
    {
        "model": "ollama/mistral",
        "base_url": "http://localhost:8000",  # litellm compatible endpoint
        "api_key": "NULL",  # placeholder
    }
]
llm_config = {"config_list": config_list}

# Setup Agents
researcher = AssistantAgent(
    name="Researcher",
    llm_config=llm_config,
    system_message="""Researcher. Extract and rank keywords based on their relevance and meaning from given texts. Specialized in ontology analysis and biology.""",
)

pubmed_master = AssistantAgent(
    name="PubMed Master",
    llm_config=llm_config,
    system_message="""PubMed Master. Your role is to generate and optimize PubMed queries by intelligently using logical operators like AND and OR. You are specialized in crafting effective search queries for biomedical literature. Your task involves creating queries in a structured 'AND/OR' format, where you will pair each extracted keyword with every other keyword in a combinatory fashion. For each pair of keywords, formulate a query using the format: (Keyword1 AND Keyword2) OR Keyword3. This approach ensures a comprehensive exploration of the research topic by covering various keyword combinations. Your goal is to generate a list of queries that effectively capture the diverse aspects of the research topic, facilitating the retrieval of the most relevant and informative articles from PubMed.""",
)

admin = UserProxyAgent(
    name="Admin",
    system_message="A human admin. Reviews and approves the plan and query created by the agents.",
    llm_config=llm_config,
    code_execution_config=False,
)

# Group Chat Setup
groupchat = GroupChat(
    agents=[admin, researcher, pubmed_master],
    messages=[],
    max_round=50,
)

manager = GroupChatManager(groupchat=groupchat, llm_config=llm_config)

# Abstract to be analyzed
abstract = "Most Staphylococcus aureus strains can grow as a multicellular biofilm, a phenotype of utmost importance to clinical infections such as endocarditis, osteomyelitis, and implanted medical device infection. As biofilms are inherently more tolerant to the host immune system and antibiotics, understanding the S. aureus genes and regulatory circuits that contribute to biofilm development is an active and ongoing field of research. This chapter details a high-throughput and standardized way to grow S. aureus biofilms using a classical microtiter plate assay. Biofilms can be quantified using crystal violet or by confocal microscopy imaging and COMSTAT analysis."

# Initiating the Chat with the Abstract
admin.initiate_chat(
    manager,
    message=f"Extract keywords from the following abstract and create a set of PubMed queries using the 'AND/OR' format: {abstract}",
)

